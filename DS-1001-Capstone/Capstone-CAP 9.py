# -*- coding: utf-8 -*-
"""1001 Capstone Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10CMyaHeRn1MuNsShzPoC-uubkoaGNuUU

# 1001 Capstone Project
Group 9 (CAP 9)

Smruti Nalawade ssn9062@nyu.edu

Hongjiu Zhang hz3501@nyu.edu

Carina Sun as13537@nyu.edu
"""

import pandas as pd
import numpy as np
import re

from scipy import stats
from scipy.stats import (
    mannwhitneyu, kruskal, ks_2samp, ttest_ind, normaltest,
    shapiro, levene, bartlett, norm, chi2_contingency, spearmanr,t
)

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.cm as cm

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    r2_score, mean_squared_error, roc_auc_score,
    roc_curve, classification_report
)
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

np.random.seed(13082093)

#load all the three datasets
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

file_path = '/content/drive/My Drive/1001 Project/'

"""# Data preprocessing"""

file_numeric = file_path + '/rmpCapstoneNum.csv'
file_qualitative = file_path + '/rmpCapstoneQual.csv'
file_tags = file_path + '/rmpCapstoneTags.csv'

data_numeric_raw = pd.read_csv(file_numeric,header=None)
data_qualitative_raw = pd.read_csv(file_qualitative,header=None)
data_tags_raw = pd.read_csv(file_tags,header=None)

#Preprocessing of data_numeric

# Adding column names
numeric_column_names = [
    "Average Rating", "Average Difficulty", "Number of Ratings",
    'Received a "pepper"?', "Proportion Recommend", "Online Class Ratings",
    "Male Gender", "Female Gender"
]
data_numeric_raw.columns = numeric_column_names

#drop NaN value if their only have gender record
data_numeric_cleaned = data_numeric_raw.dropna(subset=data_numeric_raw.columns[:-2],how='all')

#show the data
data_numeric_cleaned.info()
print(data_numeric_cleaned.head())
data_numeric_cleaned.isnull().sum()

#Preprocessing of data_qualitative_raw

# Adding column names
qualitative_column_names = ["Major/Field", "University", "US State"]
data_qualitative_raw.columns = qualitative_column_names

# #drop all the null value if needed
# data_qualitative_cleaned=data_qualitative_raw.dropna(how='all')

#show the data
data_qualitative_raw.info()
print(data_qualitative_raw.head())
data_qualitative_raw.isnull().sum()

#Preprocessing of data_tags

# Adding column names
tags_column_names = [
    "Tough grader", "Good feedback", "Respected", "Lots to read", "Participation matters",
    "Don't skip class or you will not pass", "Lots of homework", "Inspirational",
    "Pop quizzes!", "Accessible", "So many papers", "Clear grading", "Hilarious",
    "Test heavy", "Graded by few things", "Amazing lectures", "Caring", "Extra credit",
    "Group projects", "Lecture heavy"
]
data_tags_raw.columns = tags_column_names


#show the data
data_tags_raw.info()
print(data_tags_raw.head())

"""# Q1-Is there statistically significant evidence of a pro-male gender bias in student evaluations of professors?"""

#Q1
male_ratings = data_numeric_cleaned[data_numeric_cleaned['Male Gender'] == 1]['Average Rating'].dropna()
female_ratings = data_numeric_cleaned[data_numeric_cleaned['Female Gender'] == 1]['Average Rating'].dropna()

# show the mean and stand deviation
male_mean, male_std = male_ratings.mean(), male_ratings.std()
female_mean, female_std = female_ratings.mean(), female_ratings.std()

# plot the histogram and distribution
plt.figure(figsize=(8, 5))
plt.hist(male_ratings, bins=30, alpha=0.7, label='Male Professors', color='blue')
plt.hist(female_ratings, bins=30, alpha=0.7, label='Female Professors', color='orange')
plt.axvline(male_mean, color='blue', linestyle='dashed', linewidth=1, label=f'Male Mean: {male_mean:.2f}')
plt.axvline(female_mean, color='orange', linestyle='dashed', linewidth=1, label=f'Female Mean: {female_mean:.2f}')
plt.title('Distribution of Average Ratings by Gender')
plt.xlabel('Average Rating')
plt.ylabel('Frequency')
plt.legend()
plt.show()

# normality test：Kolmogorov-Smirnov test
ks_test_male = ks_2samp(male_ratings, norm.rvs(loc=male_mean, scale=male_std, size=len(male_ratings)))
ks_test_female = ks_2samp(female_ratings, norm.rvs(loc=female_mean, scale=female_std, size=len(female_ratings)))

print("Kolmogorov-Smirnov Test for Normality:")
print(f"Male Ratings: Test Statistic = {ks_test_male.statistic:.4f}, P-value = {ks_test_male.pvalue:.4e}")
print(f"Female Ratings: Test Statistic = {ks_test_female.statistic:.4f}, P-value = {ks_test_female.pvalue:.4e}")

# homogeneity test of variance
levene_test = levene(male_ratings, female_ratings)

# select the test method based on the hypothesis test result
if ks_test_male.pvalue > 0.005 and ks_test_female.pvalue > 0.005 and levene_test.pvalue > 0.005:
    # t test
    ttest_result = ttest_ind(male_ratings, female_ratings, alternative='greater')
    test_stat, p_value = ttest_result.statistic, ttest_result.pvalue
    test_type = "Two-Sample t-test"
else:
    # Mann-Whitney U test
    mannwhitney_result = mannwhitneyu(male_ratings, female_ratings, alternative='greater')
    test_stat, p_value = mannwhitney_result.statistic, mannwhitney_result.pvalue
    test_type = "Mann-Whitney U Test"

# Cohen's d
effect_size = (male_mean - female_mean) / np.sqrt((male_std**2 + female_std**2) / 2)


print(f"{test_type} Results:")
print(f"Test Statistic: {test_stat:.4f}")
print(f"P-value: {p_value:.4e}")
print(f"Effect Size (Cohen's d): {effect_size:.4f}")

alpha = 0.005
if p_value < alpha:
    print("Conclusion: There is evidence of a pro-male gender bias in ratings.")
else:
    print("Conclusion: There is no significant evidence of a pro-male gender bias in ratings.")

"""# Q2-Is there a statistically significant gender difference in the variance of professor ratings?"""

#Q2
# Calculate variance for both groups
male_variance = male_ratings.var()
female_variance = female_ratings.var()

# Perform variance homogeneity tests
levene_test = levene(male_ratings, female_ratings)
bartlett_test = bartlett(male_ratings, female_ratings)
ks_test_result = ks_2samp(male_ratings, female_ratings)

plt.figure(figsize=(8, 5))
plt.boxplot([male_ratings, female_ratings], labels=['Male Professors', 'Female Professors'])
plt.title('Variance of Average Ratings by Gender')
plt.ylabel('Average Rating')
plt.show()

#descriptive statistics
print("Descriptive Statistics:")
print(f"Male Variance: {male_variance:.4f}")
print(f"Female Variance: {female_variance:.4f}")

#results for Levene's Test
print("\nLevene’s Test for Equality of Variances:")
print(f"Test Statistic: {levene_test.statistic:.4f}")
print(f"P-value: {levene_test.pvalue:.4e}")

#results for Bartlett's Test
print("\nBartlett’s Test for Equality of Variances:")
print(f"Test Statistic: {bartlett_test.statistic:.4f}")
print(f"P-value: {bartlett_test.pvalue:.4e}")

#results for Kolmogorov-Smirnov Test
print("\nKolmogorov-Smirnov Test for Distribution Differences:")
print(f"Test Statistic: {ks_test_result.statistic:.4f}")
print(f"P-value: {ks_test_result.pvalue:.4e}")

# Interpret results
if ks_test_result.pvalue < 0.005:
    print("Conclusion: The distributions of male and female ratings are significantly different.")
else:
    print("Conclusion: There is no significant difference in the distributions of male and female ratings.")

"""# Q3-What is the likely size of both of these effects as estimated from this dataset with 95% confidence?

"""

# Q3

# Bootstrap confidence intervals for the mean difference
n_bootstrap = 10000
bootstrap_diffs = []

for _ in range(n_bootstrap):
    male_sample = np.random.choice(male_ratings, size=len(male_ratings), replace=True)
    female_sample = np.random.choice(female_ratings, size=len(female_ratings), replace=True)
    bootstrap_diffs.append(np.mean(male_sample) - np.mean(female_sample))

# Calculate 95% confidence interval from bootstrap samples
bootstrap_diffs = np.array(bootstrap_diffs)
lower_ci, upper_ci = np.percentile(bootstrap_diffs, [2.5, 97.5])
mean_diff = male_ratings.mean() - female_ratings.mean()

# Plot bootstrap distribution with confidence interval
plt.figure(figsize=(10, 6))
plt.hist(bootstrap_diffs, bins=30, alpha=0.6, color='skyblue', label='Resampled Mean Differences')
plt.axvline(mean_diff, color='red', linestyle='dashed', linewidth=2, label=f'Original Mean Difference: {mean_diff:.3f}')
plt.axvline(lower_ci, color='green', linestyle='dashed', linewidth=2, label=f'Lower CI: {lower_ci:.3f}')
plt.axvline(upper_ci, color='orange', linestyle='dashed', linewidth=2, label=f'Upper CI: {upper_ci:.3f}')
plt.title('Bootstrap Mean Difference with 95% Confidence Interval')
plt.xlabel('Mean Difference (Male - Female)')
plt.ylabel('Frequency')
plt.legend()
plt.show()

# Print results
print("Gender Bias in Average Rating:")
print(f"Mean Difference: {mean_diff:.4f}")
print(f"95% Confidence Interval for Mean Difference (Bootstrap): {lower_ci:.4f} to {upper_ci:.4f}")

# Levene’s Test for Variance Difference
levene_test = levene(male_ratings, female_ratings)
var_diff = male_ratings.var() - female_ratings.var()

# Calculate Cohen's d for mean difference
male_std = male_ratings.std()
female_std = female_ratings.std()
pooled_std = np.sqrt(((male_std ** 2) + (female_std ** 2)) / 2)
cohen_d = mean_diff / pooled_std

print("\nGender Bias in Spread of Ratings (Variance):")
print(f"Variance Difference: {var_diff:.4f}")
print(f"Levene’s Test p-value: {levene_test.pvalue:.4e}")
print(f"Effect Size (Cohen's d): {cohen_d:.4f}")

# Conclusion
if levene_test.pvalue < 0.005:
    print("\nConclusion: The variance difference is statistically significant, with female professors having greater variability in ratings.")
else:
    print("\nConclusion: There is no statistically significant difference in the variance of ratings between genders.")

"""# Q4- Is there a gender difference in the tags awarded by students?"""

#Question 4

# Reset indices to ensure alignment
data_tags_numeric = data_tags_raw
data_numeric_cleaned.reset_index(drop=True, inplace=True)
data_tags_numeric.reset_index(drop=True, inplace=True)

# Check if datasets have a common column for alignment
common_columns = set(data_numeric_cleaned.columns).intersection(data_tags_numeric.columns)
if common_columns:
    print(f"Common columns for alignment: {common_columns}")
    # Align dataframes based on a shared key
    shared_key = list(common_columns)[0]  # Use the first common column
    data_combined = pd.merge(
        data_numeric_cleaned,
        data_tags_numeric,
        on=shared_key,
        how="inner",
        suffixes=("_num", "_tag"),
    )
    print(f"After merging on {shared_key}, rows: {data_combined.shape[0]}")
else:
    print("No common columns found for alignment. Aligning by index position.")

    # Align by index position (if no common key exists)
    min_rows = min(data_numeric_cleaned.shape[0], data_tags_numeric.shape[0])
    data_numeric_cleaned = data_numeric_cleaned.iloc[:min_rows].copy()
    data_tags_numeric = data_tags_numeric.iloc[:min_rows].copy()

# Drop rows where gender information is missing
data_numeric_cleaned.dropna(subset=["Male Gender", "Female Gender"], inplace=True)
data_tags_numeric = data_tags_numeric.loc[data_numeric_cleaned.index]  # Match tags to cleaned numerica indices

# Ensure that both dataframes are the same length
assert data_numeric_cleaned.shape[0] == data_tags_numeric.shape[0], "Row counts still do not match!"
print(f"Aligned row counts: {data_numeric_cleaned.shape[0]} rows")

# Now proceed with the chi-square test
tag_p_values = {}

for tag in tags_column_names:
    # Sum up tag counts for each gender
    male_tag_counts = data_tags_numeric.loc[data_numeric_cleaned["Male Gender"] == 1, tag].sum()
    female_tag_counts = data_tags_numeric.loc[data_numeric_cleaned["Female Gender"] == 1, tag].sum()

    # Construct a 2x2 contingency table
    contingency_table = [
        [male_tag_counts, len(data_numeric_cleaned[data_numeric_cleaned["Male Gender"] == 1]) - male_tag_counts],
        [female_tag_counts, len(data_numeric_cleaned[data_numeric_cleaned["Female Gender"] == 1]) - female_tag_counts]
    ]

    # Perform Chi-Square Test
    try:
        chi2_stat, p_value, _, _ = chi2_contingency(contingency_table, correction=False)
        tag_p_values[tag] = p_value
    except ValueError as e:
        print(f"Skipping tag '{tag}' due to insufficient data.")

        # Safeguard logic: Only try to delete rows if the tag exists in the DataFrame
        if tag in data_numeric_cleaned.columns:
            data_numeric_cleaned = data_numeric_cleaned[data_numeric_cleaned[tag] == 0]
            data_tags_numeric = data_tags_numeric.loc[data_numeric_cleaned.index]
        else:
            print(f"Skipping deletion as the tag '{tag}' does not exist in the data.")

# Display results
sorted_tags = sorted(tag_p_values.items(), key=lambda x: (x[1] is None, x[1]))
print("Least Gendered Tags (Highest p-values):", sorted_tags[-3:])
print("Most Gendered Tags (Lowest p-values):", sorted_tags[:3])
valid_tags = {tag: p for tag, p in tag_p_values.items() if p is not None}
sorted_tags = sorted(valid_tags.items(), key=lambda x: x[1])
tags_sorted = [tag for tag, _ in sorted_tags]
p_values_sorted = [p for _, p in sorted_tags]
significance_threshold = 0.005
tags_sorted_reverse = [x for _, x in sorted(zip(p_values_sorted, tags_sorted), reverse=True)]
p_values_sorted_reverse = sorted(p_values_sorted, reverse=True)

# Create the bar plot with a logarithmic scale
plt.figure(figsize=(14, 7))
colors = ['blue' if p <= significance_threshold else 'gray' for p in p_values_sorted]
plt.bar(tags_sorted, p_values_sorted, color=colors)
plt.axhline(y=significance_threshold, color='red', linestyle='--', label="p = 0.005")
plt.yscale('log')
plt.ylim(1e-20, 1)  # Adjust limits for better visibility of small p-values
plt.title("P-Values for Gender Differences Across Tags (Chi-Square Test)", fontsize=16)
plt.xlabel("Tags", fontsize=12)
plt.ylabel("P-Value (log scale)", fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.legend(["p = 0.005", "Significant (p < 0.005)", "Not Significant (p >= 0.005)"], loc="upper left")
plt.tight_layout()
plt.show()

"""# Q5-Is there a gender difference in terms of average difficulty?"""

#Question 5

# Ensure gender columns exist in the data
assert "Male Gender" in data_numeric_cleaned.columns
assert "Female Gender" in data_numeric_cleaned.columns

# Separate difficulty ratings by gender using the correct column name
male_difficulty = data_numeric_cleaned.loc[data_numeric_cleaned["Male Gender"] == 1, "Average Difficulty"]
female_difficulty = data_numeric_cleaned.loc[data_numeric_cleaned["Female Gender"] == 1, "Average Difficulty"]
male_variance =   male_difficulty.var()
female_variance = female_difficulty.var()
variance_ratio = max(male_variance / female_variance, female_variance / male_variance)
equal_var = variance_ratio <= 2.0  # Assume equal variances if ratio is ≤ 2
print(f"Variance Ratio: {variance_ratio:.4f} (Equal Variance Assumption: {equal_var})")

# Perform a two-sample t-test
t_stat, p_value = ttest_ind(male_difficulty.dropna(), female_difficulty.dropna(), equal_var=equal_var)

print(f"T-Test: Statistic={t_stat:.4f}, p-value={p_value:.4f}")

# Interpret results
if p_value < 0.005:
    print("There is a statistically significant gender difference in average difficulty ratings.")
else:
    print("There is no statistically significant gender difference in average difficulty ratings.")

# Visualization: Average Difficulty Ratings by Gender
# Calculate mean and standard deviation for each gender

male_mean = male_difficulty.mean()
male_std = male_difficulty.std()
female_mean = female_difficulty.mean()
female_std = female_difficulty.std()

difficulty_means = [male_mean, female_mean]
difficulty_stds = [male_std, female_std]
genders = ["Male", "Female"]
plt.figure(figsize=(8, 6))
plt.bar(genders, difficulty_means, yerr=difficulty_stds, capsize=10, color=["blue", "orange"], alpha=0.7)
plt.axhline(y=0, color='gray', linestyle='--', linewidth=0.8)  # Baseline
if p_value < 0.005:
    plt.text(0.5, max(difficulty_means) + 0.1, "Significant Difference", color='red', fontsize=12, ha='center')
plt.title("Average Difficulty Ratings by Gender", fontsize=16)
plt.ylabel("Average Difficulty", fontsize=12)
plt.xlabel("Gender", fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.tight_layout()
plt.show()

"""# Q6-Quantify the likely size of this effect at 95% confidence"""

#Question 6

# Mean and standard deviation for each gender
male_mean = male_difficulty.mean()
female_mean = female_difficulty.mean()
male_std = male_difficulty.std()
female_std = female_difficulty.std()

print(f"Mean for Males: {male_mean:.4f}")
print(f"Mean for Females: {female_mean:.4f}")
print(f"Standard Deviation for Males: {male_std:.4f}")
print(f"Standard Deviation for Females: {female_std:.4f}")


# Sample sizes
male_n = male_difficulty.dropna().shape[0]
female_n = female_difficulty.dropna().shape[0]

print(f"Sample Size - Males: {male_n:.4f}")
print(f"Sample Size - Females: {female_n:.4f}")
mean_diff = male_mean - female_mean
print(f"Mean Difference (Male - Female): {mean_diff:.4f}")
se_diff = np.sqrt((male_std**2 / male_n) + (female_std**2 / female_n))

# Degrees of freedom (for Welch's t-test)
df = ((male_std**2 / male_n) + (female_std**2 / female_n))**2 / (
    ((male_std**2 / male_n)**2 / (male_n - 1)) + ((female_std**2 / female_n)**2 / (female_n - 1))
)

# Critical t-value for 95% CI
t_crit = t.ppf(0.975, df)
ci_lower = mean_diff - t_crit * se_diff
ci_upper = mean_diff + t_crit * se_diff

# Plotting
plt.figure(figsize=(8, 6))
bars = plt.bar(
    ['Male - Female'],  # Label for the bar
    [mean_diff],  # Mean difference value
    color='dodgerblue',  # Bar color
    yerr=[[mean_diff - ci_lower], [ci_upper - mean_diff]],  # Error bars for CI
    capsize=5  # Make the error bars visible
)
plt.title("Gender Difference in Average Difficulty (Mean ± 95% CI)", fontsize=16)
plt.ylabel("Mean Difference in Difficulty", fontsize=12)
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# Print out the Confidence Interval for clarity
print(f"95% Confidence Interval for Mean Difference: ({ci_lower:.4f}, {ci_upper:.4f})")

"""#Q7-A regression model predicting average rating from all numerical predictors."""

data = data_numeric_cleaned.dropna(how='all')
data.dropna(inplace = True)
# Separate features and target variable
X = data.drop(columns=['Average Rating'])
y = data['Average Rating']
X_with_const = sm.add_constant(data[[
    "Average Difficulty", "Number of Ratings",
    'Received a "pepper"?', "Proportion Recommend", "Online Class Ratings",
    "Male Gender", "Female Gender"
]])
# Calculate VIF to check for multicollinearity
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

threshold_moderate = 5
threshold_high = 10

# Determine colors based on thresholds
colors = np.where(vif_data["VIF"] > threshold_high, 'red',
                  np.where(vif_data["VIF"] > threshold_moderate, 'orange', 'green'))

#  VIF Plot
plt.figure(figsize=(12, 8))
bars = plt.barh(vif_data["feature"], vif_data["VIF"], color=colors, edgecolor='black', alpha=0.8)

for bar in bars:
    plt.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height() / 2,
             f'{bar.get_width():.2f}', va='center', fontsize=10)

# Add vertical lines for thresholds
plt.axvline(threshold_moderate, color='orange', linestyle='--', label='Moderate Threshold (5)')
plt.axvline(threshold_high, color='red', linestyle='--', label='High Threshold (10)')
plt.xlabel("Variance Inflation Factor (VIF)", fontsize=14)
plt.ylabel("Features", fontsize=14)
plt.title("VIF for Numerical Predictors for Average Ratings", fontsize=16)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

# Question 7
# PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA(n_components=0.95)  # Retain 95% variance
X_pca = pca.fit_transform(X_scaled)
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# Model summary using statsmodels
ols_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()
print("\nMultiple Regression Results After PCA")
print("-" * 50)
print(ols_model.summary().tables[1])

print(f"R²: {r2:.3f}")
print(f"Model RMSE: {rmse:.3f}")
results = {
    "R2": r2,
    "RMSE": rmse,
    "OLS Summary": ols_model.summary()
}

# PCA Interpretation
# Ensure PCA components and regression coefficients align
pca_loadings = pd.DataFrame(
    pca.components_,
    columns=X.columns,
    index=[f"PC{i+1}" for i in range(pca.n_components_)]
)

# Extract regression coefficients for PCA components (excluding intercept)
coefficients = ols_model.params[1:]
coefficients.index = [f"PC{i+1}" for i in range(len(coefficients))]  # Rename to match PCA components

# Compute the contribution of original features
feature_contributions = pca_loadings.T.dot(coefficients)
feature_contributions = feature_contributions.abs()
feature_contributions = feature_contributions / feature_contributions.sum()

# Display the contributions of each feature
print("\nFeature Contributions to Average Rating Prediction:")
print(feature_contributions.sort_values(ascending=False))

feature_contributions_sorted = feature_contributions.sort_values(ascending=False)

# Plot
plt.figure(figsize=(10, 6))
feature_contributions_sorted.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title("Feature Contributions to Average Rating Prediction", fontsize=14)
plt.ylabel("Contribution (Normalized)", fontsize=12)
plt.xlabel("Features", fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""# Q8-A regression model predicting average ratings from all tags"""

# Question 8
# Align normalized_tags with the indices of y
normalized_tags = data_tags_raw.div(data['Number of Ratings'], axis=0)
normalized_tags = normalized_tags.loc[y.index]

# Add constant for OLS regression
normalized_tags_with_const = sm.add_constant(normalized_tags)

# Train-test split
X_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(
    normalized_tags_with_const, y, test_size=0.2
)

tags_model = sm.OLS(y_train_tags, X_train_tags).fit()
y_pred_tags = tags_model.predict(X_test_tags)

# Metrics for Original Regression
tags_r2 = r2_score(y_test_tags, y_pred_tags)
tags_rmse = np.sqrt(mean_squared_error(y_test_tags, y_pred_tags))
coefficients = tags_model.params[1:]  # Exclude the constant term
most_predictive_tags_ols = coefficients.abs().sort_values(ascending=False)

# Remove constant for Lasso and Ridge
X_train_scaled = StandardScaler().fit_transform(X_train_tags.iloc[:, 1:])  # Exclude constant
X_test_scaled = StandardScaler().fit_transform(X_test_tags.iloc[:, 1:])

# Lasso Regression
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_train_scaled, y_train_tags)
y_pred_lasso = lasso_model.predict(X_test_scaled)
lasso_r2 = r2_score(y_test_tags, y_pred_lasso)
lasso_rmse = np.sqrt(mean_squared_error(y_test_tags, y_pred_lasso))
lasso_coefficients = pd.Series(lasso_model.coef_, index=normalized_tags.columns).sort_values(ascending=False)

# Ridge Regression
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train_scaled, y_train_tags)
y_pred_ridge = ridge_model.predict(X_test_scaled)
ridge_r2 = r2_score(y_test_tags, y_pred_ridge)
ridge_rmse = np.sqrt(mean_squared_error(y_test_tags, y_pred_ridge))
ridge_coefficients = pd.Series(ridge_model.coef_, index=normalized_tags.columns).sort_values(ascending=False)

# ----- Results Compilation -----
results = {
    "Original OLS": {
        "R2": tags_r2,
        "RMSE": tags_rmse,
        "Most Predictive Tags": most_predictive_tags_ols.head(3),
        "Least Predictive Tags": most_predictive_tags_ols.tail(3),
    },
    "Lasso": {
        "R2": lasso_r2,
        "RMSE": lasso_rmse,
        "Most Predictive Tags": lasso_coefficients.head(3),
        "Least Predictive Tags": lasso_coefficients.tail(3),
    },
    "Ridge": {
        "R2": ridge_r2,
        "RMSE": ridge_rmse,
        "Most Predictive Tags": ridge_coefficients.head(3),
        "Least Predictive Tags": ridge_coefficients.tail(3),
    },
}

# Output all results
for model_name, model_results in results.items():
    print(f"\n{model_name} Regression Results:")
    print(f"R2: {model_results['R2']:.3f}")
    print(f"RMSE: {model_results['RMSE']:.3f}")
    print("Most Predictive Tags:")
    print(model_results['Most Predictive Tags'])
    print("Least Predictive Tags:")
    print(model_results['Least Predictive Tags'])

plt.figure(figsize=(10, 6))
plt.barh(vif_data["feature"], vif_data["VIF"], color='skyblue', edgecolor='black', alpha=0.7)
plt.xlabel("Variance Inflation Factor (VIF)", fontsize=12)
plt.ylabel("Features", fontsize=12)
plt.title("VIF for Tags", fontsize=14)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Display Model Performance Metrics
print("Model Performance Metrics:")
print(f"R² (Coefficient of Determination): {r2:.3f}")
print(f"Root Mean Square Error (RMSE): {rmse:.3f}")

import matplotlib.cm as cm
import matplotlib.pyplot as plt

# Normalize values for color mapping
norm = plt.Normalize(vmin=most_predictive_tags_ols.values.min(), vmax=most_predictive_tags_ols.values.max())
cmap = cm.viridis

# Create the plot
fig, ax = plt.subplots(figsize=(14, 10))
colors = cmap(norm(most_predictive_tags_ols.values))
bars = ax.barh(most_predictive_tags_ols.index, most_predictive_tags_ols.values, color=colors, edgecolor='black')

# Add annotations for each bar
for bar in bars:
    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height() / 2,
            f'{bar.get_width():.2f}', va='center', fontsize=9)

# Add labels and title
ax.set_xlabel("Absolute Coefficient Value", fontsize=14)
ax.set_ylabel("Tags", fontsize=14)
ax.set_title("Predictive Power of Tags for Average Rating", fontsize=16)

# Add a colorbar
sm = cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])  # ScalarMappable does not require data
cbar = fig.colorbar(sm, ax=ax)  # Link the colorbar to the axis
cbar.set_label('Predictive Power', fontsize=12)

# Final touches
ax.grid(axis='x', linestyle='--', alpha=0.6)
fig.tight_layout()
plt.show()

"""# Q9-A regression model predicting average difficulty from all tags."""

# Question 9
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm

y = data['Average Difficulty']
# Align normalized_tags with the indices of y
normalized_tags = data_tags_raw.div(data['Number of Ratings'], axis=0)
normalized_tags = normalized_tags.loc[y.index]

# Add constant for OLS regression

normalized_tags_with_const = sm.add_constant(normalized_tags)
X_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(
    normalized_tags_with_const, y, test_size=0.2
)
tags_model = sm.OLS(y_train_tags, X_train_tags).fit()
y_pred_tags = tags_model.predict(X_test_tags)

tags_r2 = r2_score(y_test_tags, y_pred_tags)
tags_rmse = np.sqrt(mean_squared_error(y_test_tags, y_pred_tags))
coefficients = tags_model.params[1:]
most_predictive_tags_ols = coefficients.abs().sort_values(ascending=False)

# Remove constant for Lasso and Ridge
X_train_scaled = StandardScaler().fit_transform(X_train_tags.iloc[:, 1:])
X_test_scaled = StandardScaler().fit_transform(X_test_tags.iloc[:, 1:])

# Lasso Regression
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_train_scaled, y_train_tags)
y_pred_lasso = lasso_model.predict(X_test_scaled)
lasso_r2 = r2_score(y_test_tags, y_pred_lasso)
lasso_rmse = np.sqrt(mean_squared_error(y_test_tags, y_pred_lasso))
lasso_coefficients = pd.Series(lasso_model.coef_, index=normalized_tags.columns).sort_values(ascending=False)

# Ridge Regression
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train_scaled, y_train_tags)
y_pred_ridge = ridge_model.predict(X_test_scaled)
ridge_r2 = r2_score(y_test_tags, y_pred_ridge)
ridge_rmse = np.sqrt(mean_squared_error(y_test_tags, y_pred_ridge))
ridge_coefficients = pd.Series(ridge_model.coef_, index=normalized_tags.columns).sort_values(ascending=False)

# ----- Results Compilation -----
results = {
    "Original OLS": {
        "R2": tags_r2,
        "RMSE": tags_rmse,
        "Most Predictive Tags": most_predictive_tags_ols.head(3),
        "Least Predictive Tags": most_predictive_tags_ols.tail(3),
    },
    "Lasso": {
        "R2": lasso_r2,
        "RMSE": lasso_rmse,
        "Most Predictive Tags": lasso_coefficients.head(3),
        "Least Predictive Tags": lasso_coefficients.tail(3),
    },
    "Ridge": {
        "R2": ridge_r2,
        "RMSE": ridge_rmse,
        "Most Predictive Tags": ridge_coefficients.head(3),
        "Least Predictive Tags": ridge_coefficients.tail(3),
    },
}

# Output all results
for model_name, model_results in results.items():
    print(f"\n{model_name} Regression Results:")
    print(f"R2: {model_results['R2']:.3f}")
    print(f"RMSE: {model_results['RMSE']:.3f}")
    print("Most Predictive Tags:")
    print(model_results['Most Predictive Tags'])
    print("Least Predictive Tags:")
    print(model_results['Least Predictive Tags'])

most_predictive_tags_ols = coefficients.abs().sort_values(ascending=False)
lasso_coefficients = pd.Series(lasso_model.coef_, index=normalized_tags.columns).sort_values(ascending=False)
ridge_coefficients = pd.Series(ridge_model.coef_, index=normalized_tags.columns).sort_values(ascending=False)
most_predictive_difficulty_tags = {
    "OLS": most_predictive_tags_ols.head(3),
    "Lasso": lasso_coefficients.head(3),
    "Ridge": ridge_coefficients.head(3),
}
# Visualization of the most predictive tags for Average Difficulty
most_predictive_df = pd.DataFrame(most_predictive_difficulty_tags)

most_predictive_df.plot(kind="bar", figsize=(10, 6), color=["skyblue", "orange", "green"], edgecolor="black")
plt.title("Top 3 Most Predictive Tags for Average Difficulty Across Models", fontsize=14)
plt.ylabel("Coefficient Value", fontsize=12)
plt.xlabel("Tags", fontsize=12)
plt.xticks(rotation=0)
plt.legend(title="Models", loc="upper right", fontsize=10)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

"""# Q10-A classification model that predicts whether a professor receives a “pepper” from all available factors"""

# Question 10

y_pepper = data_numeric_raw['Received a "pepper"?']
data_numeric_raw = data_numeric_raw.dropna(subset=['Received a "pepper"?'])
y_pepper = data_numeric_raw['Received a "pepper"?']
combined_X = pd.concat([data_numeric_raw.drop(columns=['Received a "pepper"?']), normalized_tags], axis=1)
combined_X = combined_X.loc[y_pepper.index]
X_train, X_test, y_train, y_test = train_test_split(
    combined_X, y_pepper, test_size=0.2, stratify=y_pepper
)

rf_classifier = RandomForestClassifier(class_weight='balanced')
rf_classifier.fit(X_train, y_train)
y_pred = rf_classifier.predict(X_test)
y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]

# Model evaluation
roc_auc = roc_auc_score(y_test, y_pred_proba)
classification_rep = classification_report(y_test, y_pred)

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Print results
print(f"ROC AUC: {roc_auc:.2f}")
print("\nClassification Report:")
print(classification_rep)


# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})", color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.7)
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(alpha=0.5)
plt.tight_layout()
plt.show()

"""# Extra 1 - Is there a significant relationship between the number of ratings a professor receives and their average rating?"""

# Extra credit 1
# Is there a significant relationship between the number of ratings a professor receives and their average rating?

num_ratings = data_numeric_cleaned['Number of Ratings']
avg_rating = data_numeric_cleaned['Average Rating']

# Calculate Spearman correlation (does not assume linearity)
corr, p_value = spearmanr(num_ratings, avg_rating)

# Plot scatter plot
plt.figure(figsize=(8, 5))
plt.scatter(num_ratings, avg_rating, alpha=0.6, edgecolors='k')
plt.title('Relationship Between Number of Ratings and Average Rating')
plt.xlabel('Number of Ratings')
plt.ylabel('Average Rating')
plt.grid(True)
plt.show()

# Print correlation results
print(f"Spearman Correlation: {corr:.4f}")
print(f"P-value: {p_value:.4e}")
if p_value < 0.005:
    print("Conclusion: There is a significant relationship between the number of ratings and the average rating.")
else:
    print("Conclusion: No significant relationship exists between the number of ratings and the average rating.")

"""# Extra 2 - What factors most strongly influence average professor ratings, and how do these influences differ between male and female professors?"""

# E2-What factors most strongly influence average professor ratings, and how do these influences differ between male and female professors?
male_data = data_numeric_cleaned[data_numeric_cleaned['Male Gender'] == 1]
female_data = data_numeric_cleaned[data_numeric_cleaned['Female Gender'] == 1]

numerical_features = ['Average Difficulty', 'Number of Ratings', 'Proportion Recommend']

male_correlations = {}
for feature in numerical_features:
    corr, p_value = spearmanr(male_data['Average Rating'], male_data[feature], nan_policy='omit')
    male_correlations[feature] = {'Correlation': corr, 'P-Value': p_value}

female_correlations = {}
for feature in numerical_features:
    corr, p_value = spearmanr(female_data['Average Rating'], female_data[feature], nan_policy='omit')
    female_correlations[feature] = {'Correlation': corr, 'P-Value': p_value}

male_corr_df = pd.DataFrame(male_correlations).T
female_corr_df = pd.DataFrame(female_correlations).T

print("Male Professors Correlations")
print(male_corr_df)
print("\nFemale Professors Correlations")
print(female_corr_df)

features = list(male_correlations.keys())
male_corr_values = [male_correlations[f]['Correlation'] for f in features]
female_corr_values = [female_correlations[f]['Correlation'] for f in features]

x = np.arange(len(features))
width = 0.35

plt.figure(figsize=(10, 6))
plt.bar(x - width/2, male_corr_values, width, label='Male Professors')
plt.bar(x + width/2, female_corr_values, width, label='Female Professors')

plt.xticks(x, features, rotation=45)
plt.ylabel('Spearman Correlation')
plt.title('Correlation of Features with Average Rating')
plt.legend()
plt.tight_layout()
plt.show()